const video = document.getElementById("video");

Promise.all([
  faceapi.nets.tinyFaceDetector.loadFromUri("/models"),
  faceapi.nets.faceLandmark68Net.loadFromUri("/models"),
  faceapi.nets.faceRecognitionNet.loadFromUri("/models"),
  faceapi.nets.faceExpressionNet.loadFromUri("/models"),
]).then(startWebcam);

function startWebcam(){
  navigator.mediaDevices.getUserMedia({
    video: true,
    audio:false
  }).then((stream)=>{
    video.srcObject = stream;
  }).catch((error)=>{
    console.error(error);
  });
}

video.addEventListener("play", async () => {
  const canvas = faceapi.createCanvasFromMedia(video);
  document.body.append(canvas);

  const displaySize = { width: video.width, height: video.height };
  faceapi.matchDimensions(canvas, displaySize);

  setInterval(async () => {
    // face detection
    const detections = await faceapi
      .detectAllFaces(video, new faceapi.TinyFaceDetectorOptions())
      .withFaceLandmarks()
      .withFaceDescriptors();

    // resize detection according to canvas
    const resizedDetections = faceapi.resizeResults(detections, displaySize);
    const context = canvas.getContext("2d");
    context.clearRect(0, 0, canvas.width, canvas.height);

    // draw rectangle
    resizedDetections.forEach((detection) => {
      const box = detection.detection.box;
      
      context.strokeStyle = "white";
      context.lineWidth = 5.0;
      context.rect(box.x, box.y, box.width, box.height);
      drawBox.draw(canvas);

      context.font = "50px Arial";
      context.fillText(, boxX + boxW / 2 - 25, boxY - 20);
    });
  }, 100);
});
      // canvas에 detections 바탕으로 그림 그리는 함수. drawDetections는 face-api 제공인데, 이것 대신 우리가 직접 그리면 된다.
      context.strokeStyle = "white";
      context.lineWidth = 5.0;
      context.rect(boxX, boxY, boxW, boxH);
      context.stroke();

      context.font = "50px Arial";
      context.fillText(currentEmoji, boxX + boxW / 2 - 25, boxY - 20);
