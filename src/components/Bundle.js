import { useEffect, useRef, useState } from "react";
import * as faceapi from "face-api.js";

// Enabling Camera
const enableCamera = async (videoRef) => {
  try {
    const stream = await navigator.mediaDevices.getUserMedia({
      video: true,
    });
    if (videoRef.current) {
      videoRef.current.srcObject = stream;
    }
  } catch (error) {
    console.log("Error accessing the camera: " + error);
  }
};

// Disabling Camera ==> not working
const disableCamera = async (videoRef) => {
  try {
    const stream = videoRef.current.srcObject;
    const tracks = stream.getTracks();
    tracks[0].stop();
  } catch (error) {
    console.log("Error accessing the camera: " + error);
  }
};

const Bundle = ({ currentContentId, closeBundle, uploadReply }) => {
  const videoRef = useRef(null);
  const [capturedDataURL, setCapturedDataURL] = useState(null);
  const [isStreaming, setIsStreaming] = useState(true);

  // detect emojiì— ì“¸ useRef ì„ ì–¸
  const imgRef = useRef();
  const canvasRef = useRef();
  // detectingì´ failí–ˆì„ ê²½ìš°
  const [detectionFail, setDetectionFail] = useState(false);

  // emojië¥¼ ì €ìž¥í•  useRef ì„ ì–¸
  const [currentEmoji, setCurrentEmoji] = useState("");

  // biggest emotionì„ ì¶”ì¶œí•  í•¨ìˆ˜ biggestOf ì„ ì–¸
  const biggestOf = (detectedExpressions) => {
    var keys = Object.keys(detectedExpressions);
    var max = keys[0];
    var i;
    for (i = 1; i < keys.length; i++) {
      if (detectedExpressions[keys[i]] > detectedExpressions[max]) {
        max = keys[i];
      }
    }
    return max;
  };

  // emotionì„ emojië¡œ ë³€í™˜í•  ì˜¤ë¸Œì íŠ¸ mapEmoji ì„¤ì •
  const mapEmoji = {
    angry: "ðŸ‘¿",
    disgusted: "ðŸ¤¢",
    fearful: "ðŸ˜¨",
    happy: "ðŸ˜Š",
    neutral: "ðŸ˜",
    sad: "ðŸ˜¥",
    surprised: "ðŸ˜²",
  };

  // imageì—ì„œ ì–¼êµ´ì„ ì¸ì‹í•˜ì—¬ ì˜ì—­ì„ í‘œì‹œí•˜ëŠ” í•¨ìˆ˜. face-api ì‚¬ìš©.
  const handleImage = async () => {
    const detections = await faceapi
      .detectAllFaces(imgRef.current, new faceapi.TinyFaceDetectorOptions())
      .withFaceLandmarks()
      .withFaceExpressions();

    // ìº¡ì³ì™€ ë™ì‹œì— ì´ëª¨ì§€ê°€ ëœ¨ê¸° ìœ„í•´, detections arrayì— ê°ì²´ê°€ ìƒê¸´ ê²ƒì„ í™•ì¸ í›„ ì§„í–‰
    if (detections.length > 0) {
      const expressions = detections[0].expressions;
      const box = detections[0].detection.box;

      console.log(expressions);
      // currentEmojië¥¼ ìµœëŒ€ê°€ì¤‘ì¹˜ ê°ì • emojië¡œ ì„¸íŒ…
      setCurrentEmoji(mapEmoji[biggestOf(expressions)]);

      // canvas ì´ˆê¸°í™” ë° ì¤€ë¹„ ê³¼ì •
      const canvas = canvasRef.current;
      const context = canvas.getContext("2d");
      context.clearRect(0, 0, canvas.width, canvas.height);
      faceapi.matchDimensions(canvas, imgRef.current);
      const resizedDetections = faceapi.resizeResults(detections, canvas);

      // resizedDectionsëŠ” detectionsì™€ ìœ ì‚¬í•œ array. ê·¸ëŸ¬ë‚˜ ì´ë¯¸ì§€ì— ë§žê²Œ ìº”ë²„ìŠ¤ í¬ê¸° ì¡°ì •í–ˆìœ¼ë¯€ë¡œ ë°•ìŠ¤ ìœ„ì¹˜ê°’ì„ ê°€ì ¸ì˜¬ ë•ŒëŠ” resizedDectionsë¥¼ ì‚¬ìš©í•´ì•¼ í•œë‹¤.
      const boxX = resizedDetections[0].detection.box.x;
      const boxY = resizedDetections[0].detection.box.y;
      const boxW = resizedDetections[0].detection.box.width;
      const boxH = resizedDetections[0].detection.box.height;

      // canvasì— detections ë°”íƒ•ìœ¼ë¡œ ê·¸ë¦¼ ê·¸ë¦¬ëŠ” í•¨ìˆ˜. drawDetectionsëŠ” face-api ì œê³µì¸ë°, ì´ê²ƒ ëŒ€ì‹  ìš°ë¦¬ê°€ ì§ì ‘ ê·¸ë¦¬ë©´ ëœë‹¤.
      context.strokeStyle = "white";
      context.lineWidth = 5.0;
      context.rect(boxX, boxY, boxW, boxH);
      context.stroke();

      context.font = "50px Arial";
      context.fillText(currentEmoji, boxX + boxW / 2 - 25, boxY - 20);

      // faceapi.draw.drawDetections(canvas, resizedDetections);
      // faceapi.draw.drawFaceExpressions(canvas, resizedDetections);
      // faceapi.draw.drawFaceLandmarks(canvas, resizedDetections);
    }
  };

  // face-api ì‚¬ìš©ì„ ìœ„í•œ ëª¨ë¸ë“¤ì„ ë Œë”ë§ ì‹œ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜
  useEffect(() => {
    const loadModels = async () => {
      await Promise.all([
        faceapi.nets.tinyFaceDetector.loadFromUri("/models"),
        faceapi.nets.faceLandmark68Net.loadFromUri("/models"),
        faceapi.nets.faceRecognitionNet.loadFromUri("/models"),
        faceapi.nets.faceExpressionNet.loadFromUri("/models"),
      ]);
    };

    loadModels();
  }, []);

  useEffect(() => {
    enableCamera(videoRef); // Enable camera on component mount
  }, []);

  // í˜„ìž¬ ì¹´ë©”ë¼ í™”ë©´ ìº¡ì³ ë° ìº”ë²„ìŠ¤ì— ê·¸ë¦¬ê¸°
  const capturePhoto = () => {
    const video = videoRef.current;

    if (video) {
      const canvas = document.createElement("canvas");
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;

      const context = canvas.getContext("2d");
      context.drawImage(video, 0, 0, canvas.width, canvas.height);

      canvas.toBlob((blob) => {
        if (blob) {
          const reader = new FileReader();
          reader.onloadend = () => {
            const dataURL = reader.result;
            setCapturedDataURL(dataURL);
            setIsStreaming(false); // Stop streaming after capturing image
          };
          reader.readAsDataURL(blob);
        } else {
          console.log("Failed to capture image. Please try again.");
        }
      }, "image/jpeg");
    }
  };

  const retakePhoto = () => {
    setIsStreaming(true); // Resume camera streaming
    setCapturedDataURL(null); // Reset captured image data

    const video = videoRef.current;
    if (video && video.srcObject) {
      const tracks = video.srcObject.getTracks();
      tracks.forEach((track) => track.stop()); // Stop the existing tracks
      video.srcObject = null; // Clear the video source object
    }

    enableCamera(videoRef); // Re-enable the camera stream
  };

  const handleImgLoad = () => {
    handleImage();
  };

  // ë°”ë¡œ ì°ì§€ ì•Šì„ ê²½ìš°
  const notNow = () => {
    disableCamera();
    closeBundle();
  };

  // ì°ì€ ì‚¬ì§„ ì´ëª¨ì§€ ì €ìž¥ ë° ë²ˆë“¤ ë‹«ê¸°(ì´ëª¨ì§€ë¦¬ìŠ¤íŠ¸ë¡œ ë„˜ì–´ê°€ê¸°)
  const savePhoto = () => {
    // { contentId, replyId, replyEmoji, replyTxt, timestamp }
    uploadReply({
      contentId: currentContentId,
      replyId: Date.now(),
      replyEmoji: currentEmoji,
      timestamp: Date.now(),
    });
    disableCamera();
    closeBundle();
  };

  return (
    <div className="w-full h-full flex items-center justify-center bg-gray-200">
      <div>
        {/* content */}
        <div>
          {isStreaming ? (
            <video id="videoElement" autoPlay ref={videoRef}></video>
          ) : (
            <div>
              <h2>Camera Stream Stopped</h2>
              {capturedDataURL ? (
                <div>
                  <canvas
                    ref={canvasRef}
                    className="bg-transparent absolute"
                    width="940"
                    height="650"
                  />
                  <img
                    src={capturedDataURL}
                    alt="Captured"
                    ref={imgRef}
                    onLoad={handleImgLoad}
                  />
                  <button onClick={retakePhoto}>Retake</button>
                  <button onClick={savePhoto}>OK</button>
                </div>
              ) : (
                <p>No captured image available.</p>
              )}
            </div>
          )}
          {isStreaming && !capturedDataURL && (
            <button onClick={capturePhoto}>Capture and Save</button>
          )}
        </div>
        {/* close Bundle button */}
        <button
          className="bg-blue-500 text-white px-2 rounded mt-4"
          onClick={notNow}
        >
          Not now!
        </button>
      </div>
    </div>
  );
};

export default Bundle;
